processData <- function(file.path, split = 2, fileSource){
# Read file
textData <- readFile(file.path)
l <- round(length(textData$text))
stp <- round(l/split)
# Split data
for(i in 0:(split - 1)){
txtSplit <- data.frame()
if(i == (split - 1))
{
txtSplit <- data.frame(text = textData$text[(1 + i*stp):l])
#s <- paste(c("i:", (1 + i*(round(l/5))), "f:", l), sep = " ")
}
else{
txtSplit <- data.frame(text = textData$text[(1 + i*stp):((i + 1)*stp)])
#s <- paste(c("i:", (1 + i*(round(l/5))), "f:", (i + 1)*round(l/5)), sep = " ")
}
# Clean data
txtSplit$text <- cleanText(txtSplit$text)
# Make File
fileName <- paste("./", fileSource, i,".rds", sep = "")
bakeFiles(txtSplit, ngrams.chain = 5, file.name = fileName)
rm(txtSplit)
gc()
}
}
processData(blogFile, split = 5, fileSource = "Blog")
f0 <- readRDS("./Blog0.rds")
f1 <- readRDS("./Blog1.rds")
head(f0)
knitr::opts_chunk$set(echo = TRUE)
f0 <- readRDS("./Blog0.rds")
f1 <- readRDS("./Blog1.rds")
library(dplyr)
library(tm)
library(tidytext)
library(ngram)
library(tidyr)
head(f0)
f0 <- readRDS("./Blog0.rds")
f1 <- readRDS("./Blog1.rds")
res <- bind_rows(f0, f1)
head(f0)
a <- res %>% group_by(n, ngram) %>% summarise(n = n,
ngram = ngram,
freq = sum(freq))
head(a)
a[order(a$freq, decreasing = TRUE), ]
head(a[order(a$freq, decreasing = TRUE), ])
a <- res %>% group_by(ngram) %>% summarise(n = n,
ngram = ngram,
freq = sum(freq))
head(a[order(a$freq, decreasing = TRUE), ])
a <- res %>% group_by(ngram) %>% summarise(freq = sum(freq))
head(a[order(a$freq, decreasing = TRUE), ])
a <- res %>% group_by(n, ngram) %>% summarise(freq = sum(freq))
head(a[order(a$freq, decreasing = TRUE), ])
head(f0)
head(f1)
37747+37506
head(a)
a <- res %>%
group_by(n, ngram) %>%
summarise(freq = sum(freq)) %>%
data.frame() %>%
.[order(.$freq, decreasing = TRUE), ]
head(a)
f0 <- readRDS("./Blog0.rds")
f1 <- readRDS("./Blog1.rds")
f2 <- readRDS("./Blog2.rds")
f3 <- readRDS("./Blog3.rds")
res <- bind_rows(f0, f1, f2, f3)
a <- res %>%
group_by(n, ngram) %>%
summarise(freq = sum(freq)) %>%
data.frame() %>%
.[order(.$freq, decreasing = TRUE), ]
head(a)
head(filter(a, n == 3))
head(filter(a, n == 4))
head(filter(a, n == 5))
head(filter(a, n == 6))
head(filter(a, ngram == "at the end of the"))
mergeFiles <- function(file.source, number.files){
total.file <- data.frame()
for(i in 0:(number.files-1)){
new.name <- paste("./", file.source, i, ".rds", sep = "")
new.file <- readRDS(new.name)
total.file <- bind_rows(total.file, new.file)
}
merged.file <- total.file %>%
group_by(n, ngram) %>%
summarise(freq = sum(freq)) %>%
data.frame() %>%
.[order(.$freq, decreasing = TRUE), ]
rm(total.file)
gc(verbose = FALSE)
return(merged.file)
}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(tm)
library(tidytext)
library(ngram)
library(tidyr)
blogFile <- "../Data/en_US/en_US.blogs.txt"
newsFile <- "../Data/en_US/en_US.news.txt"
twitterFile <- "../Data/en_US/en_US.twitter.txt"
readFile <- function(file.path){
print("Reading File...")
start.time <- Sys.time()
textData <- readLines(file.path,
encoding = "UTF-8",
skipNul = TRUE)
textData <- data.frame(text = textData)
end.time <- Sys.time()
time.taken <- end.time - start.time
print(paste("Task completed! Ellapsted Time: ",
time.taken, sep = " "))
return(textData)
}
# Declare function for cleaning
cleanText <- function(text) {
print("Cleaning data...")
start.time <- Sys.time()
newText <- text %>%
tolower() %>%
gsub(pattern = "[^a-zA-Z\\d\\s:]",
replacement = " ") %>%
gsub(pattern = "[[:punct:]]",
replacement =  " ") %>%
gsub(pattern = "[0-9]",
replacement = " ") %>%
gsub(pattern = "\\b(?=\\w*(\\w){3}\\1)\\w+\\b",
replacement = " ",
perl = TRUE) %>%
gsub(pattern = " +",
replacement = " ")
end.time <- Sys.time()
time.taken <- end.time - start.time
print(paste("Task completed! Ellapsted Time: ",
time.taken, sep = " "))
return(newText)
}
makeNgrams <- function(dataframe, n = 2, threshold = 10){
print(paste("Making ", n, "gram...", sep = ""))
start.time <- Sys.time()
# Build ngram data frame
ng <- dataframe %>%
unnest_tokens(word, text, token = "ngrams", n = n) %>%
table() %>%
data.frame()
ng$n <- n
names(ng) <- c("ngram", "freq", "n")
ng <- ng %>%
filter(freq > threshold) %>%
.[order(.$freq, decreasing = TRUE), ]
ng$ngram <- as.character(ng$ngram)
#ng <- ng[order(ng$freq, decreasing = TRUE), ]
end.time <- Sys.time()
time.taken <- end.time - start.time
print(paste("Task completed! Ellapsted Time: ",
time.taken, sep = " "))
return(ng)
}
bakeFiles <- function(df, ngrams.chain = 2, file.name){
predFile <- data.frame()
for(i in 2:ngrams.chain){
ng <- df %>%
makeNgrams(i, threshold = 5)
predFile <- bind_rows(predFile, ng)
rm(ng)
gc()
}
print(paste("Saving File: ", file.name, sep = ""))
start.time <- Sys.time()
saveRDS(predFile, file.name)
end.time <- Sys.time()
time.taken <- end.time - start.time
print(paste("File Saved! Ellapsted Time: ",
time.taken, sep = " "))
}
makeFile <- function(df, ngrams.chain = 2, file.name){
predFile <- data.frame()
words <- c()
for(i in 1:ngrams.chain){
words <- c(words ,paste("w", i, sep = ""))
}
for(i in 2:ngrams.chain){
ng <- df %>%
separate(ngram, sep = " ", into = words)
predFile <- bind_rows(predFile, ng)
rm(ng)
gc()
}
print(paste("Saving File: ", file.name, sep = ""))
start.time <- Sys.time()
saveRDS(predFile, file.name)
end.time <- Sys.time()
time.taken <- end.time - start.time
print(paste("File Saved! Ellapsted Time: ",
time.taken, sep = " "))
}
mergeFiles <- function(file.source, number.files){
total.file <- data.frame()
for(i in 0:(number.files-1)){
new.name <- paste("./", file.source, i, ".rds", sep = "")
new.file <- readRDS(new.name)
total.file <- bind_rows(total.file, new.file)
}
merged.file <- total.file %>%
group_by(n, ngram) %>%
summarise(freq = sum(freq)) %>%
data.frame() %>%
.[order(.$freq, decreasing = TRUE), ]
rm(total.file)
gc(verbose = FALSE)
return(merged.file)
}
processData <- function(file.path, split = 2, fileSource){
# Read file
textData <- readFile(file.path)
l <- round(length(textData$text))
stp <- round(l/split)
# Split data
for(i in 0:(split - 1)){
txtSplit <- data.frame()
if(i == (split - 1))
{
txtSplit <- data.frame(text = textData$text[(1 + i*stp):l])
#s <- paste(c("i:", (1 + i*(round(l/5))), "f:", l), sep = " ")
}
else{
txtSplit <- data.frame(text = textData$text[(1 + i*stp):((i + 1)*stp)])
#s <- paste(c("i:", (1 + i*(round(l/5))), "f:", (i + 1)*round(l/5)), sep = " ")
}
# Clean data
txtSplit$text <- cleanText(txtSplit$text)
# Make File
fileName <- paste("./", fileSource, i,".rds", sep = "")
n.chain = 5
bakeFiles(txtSplit, ngrams.chain = n.chain, file.name = fileName)
rm(txtSplit)
gc()
}
data <- mergeFiles(file.source = fileSource, number.files = split)
return(data)
}
blog <- mergeFiles("Blog", 10)
news <- mergeFiles("News", 10)
twitter <- mergeFiles("Twitter", 10)
totalData <- bind_rows(blog, news, twitter) %>%
group_by(n, ngram) %>%
summarise(freq = sum(freq)) %>%
data.frame() %>%
.[order(.$freq, decreasing = TRUE), ]
head(totalData)
head(blog)
head(news)
head(twitter)
189149 + 189416 + 130582
head(totalData)
head(twitter)
189149 + 189416 + 57450
head(totalData)
makeFile(totalData, ngrams.chain = 5, "predFile.rds")
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(tidyr)
pF <- readRDS("./predFile.rds")
cleanText <- function(text) {
newText <- text %>%
tolower() %>%
gsub(pattern = "[[:punct:]]", replacement =  " ") %>%
gsub(pattern = "[0-9]", replacement = " ") %>%
gsub(pattern = "\\b(?=\\w*(\\w){3}\\1)\\w+\\b", replacement = " ", perl = TRUE) %>%
gsub(pattern = " +", replacement = " ")
return(newText)
}
prep <- function(input.Str){
# Initialize vector. Used for the string splititng into a word data frame
words <- c()
# Cleans String
cleanStr <- cleanText(input.Str)
# Counts number of words
wordCount <- length(strsplit(cleanStr, " ")[[1]])
# Populate words vector
for(i in 1:wordCount){
# Builds names following the format: "w" + i, where i = 1, 2, ..., n
words <- c(words,
paste("w", i, sep = ""))
}
# Separates the cleaned input string into a word data frame
strEval <- separate(data.frame(str = cleanStr),
str,
sep = " ",
into = words)
return(strEval)
}
ngramFilter <- function(strEval, ngram){
# Filters initial pF dataframe by the wanted ngram
subSet <- filter(pF, n == ngram)
# Calculates the input string size
inputSize <- dim(strEval)[2]
# Sequential filtering, word by word
for(j in 1:(ngram - 1)){
subSet <- subSet %>%
filter(.[,j] == strEval[, inputSize - ngram + j + 1])
}
# Calculates the probability for each sequence to occur
subSet$prob <- subSet$freq/sum(subSet$freq)
return(subSet)
}
pred <- function(input.Str){
# Prepares input string
strEval <- prep(input.Str)
# Load inital data set
subSet <- pF
# Finds the longest ngram in the data set
ngramLim <- max(subSet$n)
# Looks for a possible output prediction. Begins from the longest ngram
# available to the shortest one (bigram). The loop will quit as soon as
# a valid sequence is found.
for(i in ngramLim:2){
subSet <- ngramFilter(strEval, ngram = i)
if(dim(subSet)[1] != 0)
{
break
}
}
# If a sequence is found, suggest the word with highest probability (first
# printed word) and picks randomly a second word, following the Mass
# Probability Distribution calculated for the sequence.
if(dim(subSet)[1] != 0){
maxProb <- subSet[which.max(subSet$prob), max(subSet$n)]
randProb <- sample(subSet[,max(subSet$n)], 1, prob = subSet$prob)
output <- c(maxProb, randProb)
}
# If no valid sequence is found, suggests a period.
else{output <- "."}
return(output)
}
pred("There")
pred("There is")
pred("There is a")
pred("There is a house")
pred("There is a house in New")
head(pF)
filter(pf, n == 3)
filter(pF, n == 3)
filter(pF, n == 4)
filter(pF, n == 5)
pred("at")
pred("at the")
pred("at the end")
pred("at the end of")
library(dplyr)
library(tidyr)
pF <- readRDS("./predFile.rds")
cleanText <- function(text) {
newText <- text %>%
tolower() %>%
gsub(pattern = "[[:punct:]]", replacement =  " ") %>%
gsub(pattern = "[0-9]", replacement = " ") %>%
gsub(pattern = "\\b(?=\\w*(\\w){3}\\1)\\w+\\b", replacement = " ", perl = TRUE) %>%
gsub(pattern = " +", replacement = " ")
return(newText)
}
prep <- function(input.Str){
# Initialize vector. Used for the string splititng into a word data frame
words <- c()
# Cleans String
cleanStr <- cleanText(input.Str)
# Counts number of words
wordCount <- length(strsplit(cleanStr, " ")[[1]])
# Populate words vector
for(i in 1:wordCount){
# Builds names following the format: "w" + i, where i = 1, 2, ..., n
words <- c(words,
paste("w", i, sep = ""))
}
# Separates the cleaned input string into a word data frame
strEval <- separate(data.frame(str = cleanStr),
str,
sep = " ",
into = words)
return(strEval)
}
ngramFilter <- function(strEval, ngram){
# Filters initial pF dataframe by the wanted ngram
subSet <- filter(pF, n == ngram)
# Calculates the input string size
inputSize <- dim(strEval)[2]
# Sequential filtering, word by word
for(j in 1:(ngram - 1)){
subSet <- subSet %>%
filter(.[,j] == strEval[, inputSize - ngram + j + 1])
}
# Calculates the probability for each sequence to occur
subSet$prob <- subSet$freq/sum(subSet$freq)
return(subSet)
}
pred <- function(input.Str){
# Prepares input string
strEval <- prep(input.Str)
# Load inital data set
subSet <- pF
# Finds the longest ngram in the data set
ngramLim <- max(subSet$n)
# Looks for a possible output prediction. Begins from the longest ngram
# available to the shortest one (bigram). The loop will quit as soon as
# a valid sequence is found.
for(i in ngramLim:2){
subSet <- ngramFilter(strEval, ngram = i)
if(dim(subSet)[1] != 0)
{
break
}
}
# If a sequence is found, suggest the word with highest probability (first
# printed word) and picks randomly a second word, following the Mass
# Probability Distribution calculated for the sequence.
if(dim(subSet)[1] != 0){
maxProb <- subSet[which.max(subSet$prob), max(subSet$n)]
randProb <- sample(subSet[,max(subSet$n)], 1, prob = subSet$prob)
output <- c(maxProb, randProb)
}
# If no valid sequence is found, suggests a period.
else{output <- "."}
return(output)
}
pred("There is a")
prep <- function(input.Str){
# Initialize vector. Used for the string splititng into a word data frame
words <- c()
# Cleans String
cleanStr <- cleanText(input.Str)
# Counts number of words
wordCount <- length(strsplit(cleanStr, " ")[[1]])
# Populate words vector
for(i in 1:wordCount){
# Builds names following the format: "w" + i, where i = 1, 2, ..., n
words <- c(words,
paste("w", i, sep = ""))
}
# Separates the cleaned input string into a word data frame
strEval <- separate(data.frame(str = cleanStr),
str,
sep = " ",
into = words)
print(strEval)
return(strEval)
}
ngramFilter <- function(strEval, ngram){
# Filters initial pF dataframe by the wanted ngram
subSet <- filter(pF, n == ngram)
# Calculates the input string size
inputSize <- dim(strEval)[2]
# Sequential filtering, word by word
for(j in 1:(ngram - 1)){
subSet <- subSet %>%
filter(.[,j] == strEval[, inputSize - ngram + j + 1])
}
# Calculates the probability for each sequence to occur
subSet$prob <- subSet$freq/sum(subSet$freq)
return(subSet)
}
pred <- function(input.Str){
# Prepares input string
strEval <- prep(input.Str)
# Load inital data set
subSet <- pF
# Finds the longest ngram in the data set
ngramLim <- max(subSet$n)
# Looks for a possible output prediction. Begins from the longest ngram
# available to the shortest one (bigram). The loop will quit as soon as
# a valid sequence is found.
for(i in ngramLim:2){
subSet <- ngramFilter(strEval, ngram = i)
if(dim(subSet)[1] != 0)
{
break
}
}
# If a sequence is found, suggest the word with highest probability (first
# printed word) and picks randomly a second word, following the Mass
# Probability Distribution calculated for the sequence.
if(dim(subSet)[1] != 0){
maxProb <- subSet[which.max(subSet$prob), max(subSet$n)]
randProb <- sample(subSet[,max(subSet$n)], 1, prob = subSet$prob)
output <- c(maxProb, randProb)
}
# If no valid sequence is found, suggests a period.
else{output <- "."}
return(output)
}
pred("There")
pred("There is")
pred("There is a house in New")
makeFile(totalData, ngrams.chain = 5, "./predFile.rds")
pF <- readRDS("./predFile.rds")
pred("There")
pF <- readRDS("./predFile.rds")
gc()
gc()
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(tidyr)
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(tidyr)
pF <- readRDS("./predFile.rds")
